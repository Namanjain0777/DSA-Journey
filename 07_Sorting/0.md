# ğŸ”¢ Selection Sort

## ğŸ“Œ Definition
Selection Sort is a **comparison-based, in-place sorting algorithm** where we repeatedly:

> Select the **minimum element** from the unsorted part of the array and **swap it with the first unsorted element**.

---

## ğŸ” How Selection Sort Works

1. Divide the array into two parts:
   - **Sorted part** (left)
   - **Unsorted part** (right)

2. Find the **minimum element** in the unsorted part
3. Swap it with the **first element of the unsorted part**
4. Move the boundary of the sorted part by one
5. Repeat until the array is sorted

---

## ğŸ“Š Example Dry Run

### Input Array
[64, 25, 12, 22, 11]


### Pass 1
- Minimum = 11  
- Swap with 64
[11, 25, 12, 22, 64]


### Pass 2
- Minimum = 12
[11, 12, 25, 22, 64]


### Pass 3
- Minimum = 22
[11, 12, 22, 25, 64]


### Pass 4
- Minimum = 25
[11, 12, 22, 25, 64]


âœ… Array is sorted

---
```
## ğŸ§  Algorithm (Pseudo Code)

for i = 0 to n-2

minIndex = i

for j = i+1 to n-1

if arr[j] < arr[minIndex]

minIndex = j

swap(arr[i], arr[minIndex])
```

---

## ğŸ’» Java Implementation

```java
â± Time Complexity
Case	Complexity
Best Case	O(nÂ²)
Average Case	O(nÂ²)
Worst Case	O(nÂ²)
Reason: Always performs n(nâˆ’1)/2 comparisons

Space Complexity
O(1)
âœ” In-place sorting
âœ” No extra memory used

Stability
Not Stable

Equal elements may change their relative order due to swapping

Comparisons & Swaps
Comparisons: Always O(nÂ²)

Swaps: At most n - 1 (less swaps than Bubble Sort)

Advantages

Simple and easy to understand
Performs well on small datasets
Minimal number of swaps
In-place algorithm

Disadvantages

Poor performance on large datasets
Not adaptive
Not stable
Always O(nÂ²) time complexity

When to Use Selection Sort

When memory space is very limited
When number of swaps must be minimized
For educational purposes
For very small arrays
```

# ğŸ”µ Bubble Sort

## ğŸ“Œ Definition
Bubble Sort is a **comparison-based, in-place sorting algorithm** where adjacent elements are repeatedly compared and swapped if they are in the wrong order.

> The largest element â€œbubbles upâ€ to the end of the array in each pass.

---

## ğŸ” How Bubble Sort Works

1. Start from the first element
2. Compare adjacent elements
3. Swap if the left element is greater than the right
4. After one full pass, the **largest element reaches the end**
5. Repeat the process for remaining elements
6. Stop early if no swaps occur (optimized version)

---

## ğŸ“Š Example Dry Run

### Input Array
[5, 1, 4, 2, 8]


### Pass 1


(5,1) â†’ swap â†’ [1,5,4,2,8]
(5,4) â†’ swap â†’ [1,4,5,2,8]
(5,2) â†’ swap â†’ [1,4,2,5,8]
(5,8) â†’ no swap


Largest element `8` is fixed at the end.

---

### Pass 2


[1,4,2,5,8]
(4,2) â†’ swap â†’ [1,2,4,5,8]


---

### Pass 3


[1,2,4,5,8]
(no swaps)


âœ… Array is sorted

---

## ğŸ§  Algorithm (Pseudo Code)


```
for i = 0 to n-2

swapped = false

for j = 0 to n-i-2

if arr[j] > arr[j+1]

swap(arr[j], arr[j+1])

swapped = true

if swapped == false

break
```
â± Time Complexity

Case-------------Complexity

Best Case----------O(n) (already sorted, optimized)

Average Case-------O(nÂ²)

Worst Case--------O(nÂ²)

Space Complexity
O(1)


âœ” In-place algorithm

âœ” No extra memory required

ğŸ”„ Stability

âœ… Stable Sorting Algorithm

Equal elements retain their relative order

âš– Comparisons & Swaps

Comparisons: O(nÂ²)

Swaps: Can be many (more than Selection Sort)

âœ… Advantages

Simple and easy to implement

Stable sorting algorithm

Good for small or nearly sorted arrays

Can stop early (optimized)

âŒ Disadvantages

Very slow for large datasets

Not suitable for performance-critical applications

ğŸ“ When to Use Bubble Sort

Educational purposes

Very small datasets

Nearly sorted arrays (optimized version)

# ğŸŸ¢ Insertion Sort

## ğŸ“Œ Definition
Insertion Sort is a **comparison-based, in-place, stable sorting algorithm** where elements are sorted **one by one** by inserting each element into its correct position in the already sorted part of the array.

> It works the same way you sort playing cards in your hand.

---

## ğŸ” How Insertion Sort Works

1. Assume the first element is already sorted
2. Take the next element (key)
3. Compare it with elements in the sorted part
4. Shift all larger elements one position to the right
5. Insert the key at its correct position
6. Repeat until the array is sorted

---

## ğŸ“Š Example Dry Run

### Input Array
[8, 3, 5, 2]


### Pass 1 (key = 3)


[8 | 3, 5, 2]
â†’ shift 8
â†’ [3, 8, 5, 2]


### Pass 2 (key = 5)


[3, 8 | 5, 2]
â†’ shift 8
â†’ [3, 5, 8, 2]


### Pass 3 (key = 2)


[3, 5, 8 | 2]
â†’ shift 8, 5, 3
â†’ [2, 3, 5, 8]


âœ… Array is sorted

---

## ğŸ§  Algorithm (Pseudo Code)

```

for i = 1 to n-1

key = arr[i]

j = i - 1

while j >= 0 and arr[j] > key

arr[j + 1] = arr[j]

j--

arr[j + 1] = key
```
â± Time Complexity
Case	    Complexity

Best Case	O(n) (already sorted)

Average Case	O(nÂ²)

Worst Case	O(nÂ²) (reverse sorted)

ğŸ“¦ Space Complexity
O(1)


âœ” In-place sorting
âœ” No extra memory required

ğŸ”„ Stability

âœ… Stable Sorting Algorithm

Equal elements retain their relative order

âš– Comparisons & Shifts

Comparisons: O(nÂ²)

Shifts: More shifts, but fewer swaps

Efficient for nearly sorted arrays

âœ… Advantages

Simple and easy to implement

Stable sorting algorithm

Efficient for small datasets

Performs very well on nearly sorted arrays

Adaptive (best case O(n))

âŒ Disadvantages

Poor performance on large datasets

Not suitable when input size is large

ğŸ“ When to Use Insertion Sort

When array is small

When array is nearly sorted

When stability is required

For online sorting (data coming continuously)

# ğŸ” Cycle Sort

## ğŸ“Œ Definition
Cycle Sort is an **in-place, comparison-based sorting algorithm** that minimizes the **number of writes (swaps)**.

> Each element is placed directly into its correct position by following cycles.

---

## ğŸ§  Key Idea
- Find the **correct position** of an element
- Place it there
- This displaces another element
- Continue until the cycle completes

ğŸ“Œ **Minimum number of writes = O(n)**  
ğŸ“Œ Very useful when **write operations are costly**

---

## ğŸ” How Cycle Sort Works

1. For each index `i`, treat `arr[i]` as the start of a cycle
2. Count how many elements are smaller than `arr[i]`
3. That count gives the **correct position** of the element
4. Place the element at its correct position
5. Repeat for displaced elements until the cycle ends
6. Move to the next index

---

## ğŸ“Š Example Dry Run

### Input Array
[3, 1, 5, 4, 2]


### Step 1 (cycle start = 3)
- Elements smaller than 3 â†’ {1,2} â†’ position = 2
- Place 3 at index 2



[3, 1, 3, 4, 2]


Now displaced element = 5

---

### Continue cycle
- Correct position of 5 = index 4


[3, 1, 3, 4, 5]


Displaced = 2

---

### Continue
- Correct position of 2 = index 1


[3, 2, 3, 4, 5]


Displaced = 1

---

### Continue
- Correct position of 1 = index 0


[1, 2, 3, 4, 5]


âœ… Cycle complete  
âœ… Array sorted

---

## ğŸ§  Algorithm (Pseudo Code)


```
for cycleStart = 0 to n-2
item = arr[cycleStart]
pos = cycleStart

for i = cycleStart+1 to n-1
    if arr[i] < item
        pos++

if pos == cycleStart
    continue

while item == arr[pos]
    pos++

swap(item, arr[pos])

while pos != cycleStart
    pos = cycleStart
    for i = cycleStart+1 to n-1
        if arr[i] < item
            pos++

    while item == arr[pos]
        pos++

    swap(item, arr[pos])
```

```
â± Time Complexity
Case	Complexity
Best Case	O(nÂ²)
Average Case	O(nÂ²)
Worst Case	O(nÂ²)

ğŸ“Œ Due to nested loops for position calculation

ğŸ“¦ Space Complexity
O(1)


âœ” In-place sorting
âœ” No extra memory used

ğŸ”„ Stability

âŒ Not Stable

Relative order of equal elements is not preserved

âš– Comparisons & Writes

Comparisons: O(nÂ²)

Writes (swaps): O(n) (MINIMUM among comparison sorts)

ğŸ”¥ Main strength of Cycle Sort

âœ… Advantages

Minimum number of writes

In-place algorithm

Useful when memory write operations are expensive

Simple idea once understood

âŒ Disadvantages

Poor time complexity

Not stable

Harder to understand than Bubble / Insertion

Rarely used in real-world applications

ğŸ“ When to Use Cycle Sort

When write operations are costly (e.g., EEPROM, flash memory)

When minimizing swaps is more important than speed

For interview conceptual questions
```

# ğŸ”€ Merge Sort

## ğŸ“Œ Definition
Merge Sort is a **divide-and-conquer, comparison-based sorting algorithm** that:

> Recursively divides the array into halves, sorts them, and then **merges** the sorted halves.

---

## ğŸ§  Key Idea (Divide & Conquer)

1. **Divide** the array into two halves
2. **Conquer** by recursively sorting both halves
3. **Merge** the two sorted halves into one sorted array

ğŸ“Œ Unlike Bubble/Insertion/Selection, Merge Sort is **efficient for large datasets**.

---

## ğŸ” How Merge Sort Works

1. If array size is 1 â†’ already sorted
2. Divide array into left and right halves
3. Recursively apply merge sort on both halves
4. Merge the two sorted halves

---

## ğŸ“Š Example Dry Run

### Input Array
[8, 3, 5, 2, 7, 6]


### Step 1: Divide
[8, 3, 5] [2, 7, 6]


### Step 2: Divide further
[8] [3,5] [2] [7,6]


### Step 3: Sort & Merge
[3,5] â†’ [3,5]
[7,6] â†’ [6,7]


### Step 4: Final Merge
[3,5,8] + [2,6,7] â†’ [2,3,5,6,7,8]


âœ… Array is sorted

---

## ğŸ§  Algorithm (Pseudo Code)
```
mergeSort(arr, left, right):
if left >= right:
return

mid = (left + right) / 2
mergeSort(arr, left, mid)
mergeSort(arr, mid+1, right)
merge(arr, left, mid, right)
```
```
â± Time Complexity
Case	Complexity
Best Case	O(n log n)
Average Case	O(n log n)
Worst Case	O(n log n)

ğŸ“Œ Always divides array into halves â†’ very consistent performance

ğŸ“¦ Space Complexity
O(n)


âŒ Not in-place
âœ” Uses extra temporary arrays

ğŸ”„ Stability

âœ… Stable Sorting Algorithm

Equal elements maintain their relative order

âš– Comparisons & Performance

Much faster than Bubble / Insertion / Selection

Preferred for large datasets

Used internally in many libraries (conceptually)

âœ… Advantages

Guaranteed O(n log n) time complexity

Stable sorting algorithm

Works very well for large inputs

Divide & Conquer based

âŒ Disadvantages

Requires extra memory

Not in-place

Slower than Quick Sort in practice (due to memory overhead)

ğŸ“ When to Use Merge Sort

When stable sorting is required

When working with large datasets

When worst-case performance matters

In external sorting (large files)
```

# âš¡ Quick Sort

## ğŸ“Œ Definition
Quick Sort is a **divide-and-conquer, comparison-based sorting algorithm** that works by:

> Selecting a **pivot** element and partitioning the array such that  
> elements smaller than pivot come before it, and larger elements come after it.

---

## ğŸ§  Key Idea (Partitioning)

1. Choose a **pivot** (first / last / random / median)
2. Rearrange elements:
   - Left of pivot â†’ smaller elements
   - Right of pivot â†’ larger elements
3. Pivot is now in its **correct position**
4. Recursively apply Quick Sort on left and right subarrays

---

## ğŸ” How Quick Sort Works

1. Pick a pivot element
2. Partition the array around the pivot
3. Recursively sort left subarray
4. Recursively sort right subarray
5. Stop when subarray size â‰¤ 1

---

## ğŸ“Š Example Dry Run

### Input Array
[8, 3, 5, 2, 7, 6]


### Step 1: Choose Pivot = 6
[3, 5, 2] 6 [8, 7]


### Step 2: Sort left part
[2, 3, 5]


### Step 3: Sort right part
[7, 8]


### Final Array
[2, 3, 5, 6, 7, 8]


âœ… Sorted

---

## ğŸ§  Algorithm (Pseudo Code)
```
quickSort(arr, low, high):
if low < high:
p = partition(arr, low, high)
quickSort(arr, low, p - 1)
quickSort(arr, p + 1, high)
```



## ğŸ§© Partition Logic (Lomuto Partition)
```
pivot = arr[high]
i = low - 1

for j = low to high-1:
if arr[j] < pivot:
i++
swap(arr[i], arr[j])

swap(arr[i+1], arr[high])
return i+1
```
```
â± Time Complexity
Case	Complexity
Best Case	O(n log n)
Average Case	O(n log n)
Worst Case	O(nÂ²)

ğŸ“Œ Worst case occurs when pivot is always smallest or largest
(e.g., already sorted array with bad pivot choice)

ğŸ“¦ Space Complexity
O(log n)  (average â€“ recursion stack)
O(n)      (worst case)


âœ” In-place sorting (no extra arrays)

ğŸ”„ Stability

âŒ Not Stable

Equal elements may change their relative order

âš– Comparisons & Performance

Faster than Merge Sort in practice

Cache-friendly

Widely used in real systems

âœ… Advantages

Very fast on average

In-place algorithm

Low constant factors

Works well for large datasets

âŒ Disadvantages

Worst-case O(nÂ²)

Not stable

Performance depends on pivot choice

ğŸ“ When to Use Quick Sort

When average-case performance matters

When memory usage should be minimal

When stability is NOT required
```

# ğŸ”¥ Tim Sort (Hybrid Sorting Algorithm)

## ğŸ“Œ Definition
Tim Sort is a **hybrid, stable sorting algorithm** derived from:

> **Merge Sort + Insertion Sort**

It is the **default sorting algorithm** used in:
- Java (`Arrays.sort()` for objects)
- Python (`sorted()`, `.sort()`)

---

## ğŸ§  Key Idea (Why Tim Sort?)

Tim Sort is designed for **real-world data**, which is often:
- Partially sorted
- Contains patterns (runs)

Instead of blindly sorting everything, Tim Sort:
- Detects **already sorted subarrays (runs)**
- Uses **Insertion Sort** for small runs
- Uses **Merge Sort** to merge runs efficiently

---

## ğŸ” How Tim Sort Works

1. Divide the array into **small blocks called RUNS**
2. Sort each run using **Insertion Sort**
3. Merge sorted runs using **Merge Sort**
4. Apply special rules to maintain balanced merging

ğŸ“Œ Default run size â‰ˆ **32 or 64**

---

## ğŸ“Š Example (Conceptual)

### Input Array
[5, 6, 7, 1, 2, 3, 9, 8]


### Step 1: Identify runs
[5, 6, 7] [1, 2, 3] [9] [8]


### Step 2: Sort runs (Insertion Sort)
[5, 6, 7] [1, 2, 3] [8, 9]


### Step 3: Merge runs (Merge Sort)
[1, 2, 3, 5, 6, 7, 8, 9]


âœ… Sorted

---

## ğŸ§  Why Insertion Sort + Merge Sort?

| Algorithm | Used For |
|---------|---------|
| Insertion Sort | Small runs (fast for small / nearly sorted data) |
| Merge Sort | Efficient merging of large sorted runs |

---

## ğŸ§  Algorithm (High Level Pseudo Code)
```
minRun = 32

for each run of size minRun:
insertionSort(run)

while number of runs > 1:
merge adjacent runs
```
```
â± Time Complexity
Case	Complexity
Best Case	O(n)
Average Case	O(n log n)
Worst Case	O(n log n)

ğŸ”¥ Best case O(n) when data is already sorted

ğŸ“¦ Space Complexity
O(n)


âŒ Not in-place
âœ” Uses temporary arrays for merging

ğŸ”„ Stability

âœ… Stable Sorting Algorithm

Maintains relative order of equal elements

âš– Comparisons & Performance

Faster than Quick Sort on real-world data

Exploits existing order

Very efficient for partially sorted arrays

âœ… Advantages

Very fast in practice

Stable sorting algorithm

Excellent for real-world datasets

Adaptive (takes advantage of sorted data)

Guaranteed O(n log n) worst case

âŒ Disadvantages

Complex implementation

Requires extra memory

Not ideal for learning basic sorting concepts

ğŸ“ When to Use Tim Sort

General-purpose sorting

When stability is required

When data is partially sorted

Large real-world datasets
```
# ğŸ”¢ Counting Sort

## ğŸ“Œ Definition
Counting Sort is a **non-comparison-based, stable sorting algorithm** that works by:

> Counting the frequency of each element and using those counts to place elements in their correct sorted position.

ğŸ“Œ It works best when the **range of input values is small**.

---

## ğŸ§  Key Idea

- Do NOT compare elements
- Count how many times each value appears
- Use prefix sums to determine final positions
- Place elements directly into output array

---

## ğŸ” How Counting Sort Works

1. Find the **maximum (and minimum)** element in the array
2. Create a **count array** of size `(max + 1)`
3. Store frequency of each element
4. Convert frequency array to **prefix sum**
5. Place elements into output array using prefix sum
6. Copy output array back to original array

---

## ğŸ“Š Example Dry Run

### Input Array
[4, 2, 2, 8, 3, 3, 1]


### Step 1: Find max
max = 8


### Step 2: Count frequency
Index (value): 0 1 2 3 4 5 6 7 8
Count array : 0 1 2 2 1 0 0 0 1


### Step 3: Prefix sum
Count array: 0 1 3 5 6 6 6 6 7


### Step 4: Build output
[1, 2, 2, 3, 3, 4, 8]


âœ… Sorted

---

## ğŸ§  Algorithm (Pseudo Code)
```
countingSort(arr):
find max element
create count[max+1]

for each element x in arr:
    count[x]++

for i = 1 to max:
    count[i] += count[i-1]

create output array

for i = n-1 to 0:
    output[count[arr[i]] - 1] = arr[i]
    count[arr[i]]--

copy output to arr
```
```
â± Time Complexity
Case	Complexity
Best Case	O(n + k)
Average Case	O(n + k)
Worst Case	O(n + k)

ğŸ“Œ n = number of elements
ğŸ“Œ k = range of input values

ğŸ“¦ Space Complexity
O(n + k)


âŒ Not in-place
âœ” Uses extra arrays

ğŸ”„ Stability

âœ… Stable Sorting Algorithm

Maintains relative order of equal elements

âš– Comparison-Based?

âŒ NO

Counting Sort does not compare elements
Thatâ€™s why it can beat O(n log n) lower bound

âœ… Advantages

Very fast for small ranges

Stable sorting algorithm

Linear time complexity

Great for integers / characters

âŒ Disadvantages

Uses extra memory

Not suitable for large ranges

Only works with non-negative integers (basic version)

ğŸ“ When to Use Counting Sort

When input range is small

When stability is required

When sorting integers or characters

As a subroutine in Radix Sort
```

# ğŸª£ Bucket Sort

## ğŸ“Œ Definition
Bucket Sort is a **distribution-based, non-comparison sorting algorithm** that works by:

> Distributing elements into a number of **buckets**, sorting each bucket individually, and then **concatenating** all buckets.

ğŸ“Œ Works best when input is **uniformly distributed** over a range.

---

## ğŸ§  Key Idea

- Divide the input range into several buckets
- Scatter elements into appropriate buckets
- Sort each bucket (usually using Insertion Sort)
- Combine all buckets to get the final sorted array

---

## ğŸ” How Bucket Sort Works

1. Create `k` empty buckets
2. Map each element to a bucket using a bucket index function
3. Sort individual buckets
4. Concatenate all buckets in order

---

## ğŸ“Š Example Dry Run

### Input Array
[0.42, 0.32, 0.23, 0.52, 0.25, 0.47, 0.51]


### Step 1: Create buckets
Bucket 0: []
Bucket 1: []
Bucket 2: []
Bucket 3: []
Bucket 4: []


### Step 2: Distribute elements
Bucket 1: [0.23]
Bucket 2: [0.32, 0.25]
Bucket 3: [0.42, 0.47]
Bucket 4: [0.52, 0.51]


### Step 3: Sort buckets
Bucket 2 â†’ [0.25, 0.32]
Bucket 3 â†’ [0.42, 0.47]
Bucket 4 â†’ [0.51, 0.52]


### Step 4: Concatenate
[0.23, 0.25, 0.32, 0.42, 0.47, 0.51, 0.52]


âœ… Sorted

---

## ğŸ§  Algorithm (Pseudo Code)
```
bucketSort(arr):
create k empty buckets
```
```
for each element x in arr:
    index = bucketIndex(x)
    add x to bucket[index]

for each bucket:
    sort bucket

concatenate all buckets
```
```
â± Time Complexity
Case	Complexity
Best Case	O(n + k)
Average Case	O(n + k)
Worst Case	O(nÂ²)

ğŸ“Œ Worst case occurs when all elements go into one bucket

ğŸ“¦ Space Complexity
O(n + k)


âŒ Not in-place
âœ” Uses extra buckets

ğŸ”„ Stability

âœ… Stable (if bucket sorting method is stable)

âš– Comparison-Based?

âŒ NO (distribution-based)

âœ… Advantages

Very fast for uniformly distributed data

Linear time in best/average case

Stable sorting (with stable bucket sort)

Useful for floating-point numbers

âŒ Disadvantages

Poor performance if data is not uniformly distributed

Extra memory usage

Needs knowledge of data range

ğŸ“ When to Use Bucket Sort

When input is uniformly distributed

When sorting floating-point numbers

When range is known and limited

When stability is required

```

# ğŸ”¢ Radix Sort

## ğŸ“Œ Definition
Radix Sort is a **non-comparison-based, stable sorting algorithm** that sorts numbers **digit by digit**.

> Instead of comparing elements, it processes digits from **least significant digit (LSD)** to **most significant digit (MSD)** using a stable sub-sort (usually Counting Sort).

---

## ğŸ§  Key Idea

- Sort numbers by **ones place**
- Then **tens place**
- Then **hundreds place**
- Continue until the largest digit is processed

ğŸ“Œ Stability of the sub-sort is **mandatory**.

---

## ğŸ” How Radix Sort Works (LSD Method)

1. Find the **maximum number** to know the number of digits
2. Perform **Counting Sort** on each digit
3. Start from **least significant digit**
4. Move towards most significant digit

---

## ğŸ“Š Example Dry Run

### Input Array
[170, 45, 75, 90, 802, 24, 2, 66]


### Pass 1 (ones place)
[170, 90, 802, 2, 24, 45, 75, 66]


### Pass 2 (tens place)
[802, 2, 24, 45, 66, 170, 75, 90]


### Pass 3 (hundreds place)
[2, 24, 45, 66, 75, 90, 170, 802]


âœ… Sorted

---

## ğŸ§  Algorithm (Pseudo Code)
```
radixSort(arr):
max = getMax(arr)

for exp = 1; max/exp > 0; exp *= 10:
    countingSortByDigit(arr, exp)
```

```
â± Time Complexity
O(d Ã— (n + k))


Where:

n = number of elements

d = number of digits

k = base (10 for decimal)

ğŸ“Œ Often written as O(n) when d is small.

ğŸ“¦ Space Complexity
O(n + k)


âŒ Not in-place
âœ” Uses extra arrays

ğŸ”„ Stability

âœ… Stable Sorting Algorithm

Required for correct Radix Sort

âš– Comparison-Based?

âŒ NO

Radix Sort does not compare elements

âœ… Advantages

Linear time for fixed digit length

Stable sorting

Very efficient for large integers

Works well when digit count is small

âŒ Disadvantages

Extra memory usage

Only works with integers / fixed-length keys

Slower if numbers have many digits

ğŸ“ When to Use Radix Sort

Sorting large lists of integers

When range is large but digit count is small

When stability is required

Used internally in high-performance systems
```